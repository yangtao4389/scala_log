#### 第二种配置
参考：<a href ="https://github.com/heibaiying/BigData-Notes/blob/master/notes/Flume%E6%95%B4%E5%90%88Kafka.md"  >教程</a>

1. 开一个终端页面启动kafka
```shell
bin/kafka-server-start.sh config/server.properties
```
也可以在后台运行
```shell
# nohup bin/kafka-server-start.sh config/server.properties &
```

2. 创建一个主题 `flume-kafka`，之后 Flume 收集到的数据都会发到这个主题上：
```shell
# 创建主题
bin/kafka-topics.sh --create \
--zookeeper hadoop001:2181 \
--replication-factor 1   \
--partitions 1 --topic flume-kafka

# 查看创建的主题
bin/kafka-topics.sh --zookeeper hadoop001:2181 --list
# 删除主题
bin/kafka-topics.sh --zookeeper hadoop001:2181 --delete --topic flume-kafka
# 查看详情
bin/kafka-topics.sh --zookeeper hadoop001:2181  --describe --topic flume-kafka
```

3. 启动kafka消费，关联主题flume-kafka
 ```shell
 bin/kafka-console-consumer.sh --bootstrap-server hadoop001:9092 --topic flume-kafka
 ```
 启动后并不会有任何反应，暂时
 
4. 配置flume   
新建配置文件 `exec-memory-kafka.properties`，文件内容如下。这里我们监听一个名为 `/home/code/dev/scala_log/nginx/access.log` 的文件，当文件内容有变化时，将新增加的内容发送到 Kafka 的 `flume-kafka` 主题上。

  
```properties
a1.sources = s1
a1.channels = c1
a1.sinks = k1                                                                                         

a1.sources.s1.type=exec
# 该文件必须先存在
a1.sources.s1.command=tail -F /home/code/dev/scala_log/nginx/access.log
a1.sources.s1.channels=c1 

#设置Kafka接收器
a1.sinks.k1.type= org.apache.flume.sink.kafka.KafkaSink
#设置Kafka地址
a1.sinks.k1.brokerList=hadoop001:9092
#设置发送到Kafka上的主题
a1.sinks.k1.topic=flume-kafka
#设置序列化方式
a1.sinks.k1.serializer.class=kafka.serializer.StringEncoder
a1.sinks.k1.channel=c1     

a1.channels.c1.type=memory
a1.channels.c1.capacity=10000
a1.channels.c1.transactionCapacity=100  
```
5. 启动Flume

```shell
flume-ng agent \
--conf conf \
--conf-file /home/code/dev/scala_log/HOW_TO_START/exec-memory-kafka.properties \
--name a1 -Dflume.root.logger=INFO,console
```

6. 测试
向监听的 `/home/code/dev/scala_log/nginx/access.log` 文件中追加内容，查看 Kafka 消费者的输出：
 ```sbtshell
python /home/code/dev/scala_log/generate_log.py
```
7. 使用定时器去跑测试案例
* 查看是否启动`ps aux|grep cron`
* 编辑：`crontab -e`  
`*/1 * * * * python /home/code/dev/scala_log/generate_log.py`每分钟一次
通过3 来查看

 
#### 错误
1. 启动flume-ng 找不到或无法加载主类 org.apache.hadoop.hbase.util.GetJavaProperty
* 该错误可以忽略